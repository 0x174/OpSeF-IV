{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook based on \"segmentation_example_keras\" and \"solution\" notebooks\n",
    "the EMBL Machine Learning for Image Analysis Course 2018:\n",
    "\n",
    "Organisers \n",
    "Anna Kreshuk - EMBL, Vera Matser - EMBL EBI, Tobias Rasse - EMBL\n",
    "\n",
    "Trainers\n",
    "Thorsten Falk - Freiburg University\n",
    "Fred Hamprecht - Heidelberg University\n",
    "Anna Kreshuk - EMBL\n",
    "Constantin Pape - Heidelberg University\n",
    "Tobias Rasse - EMBL\n",
    "Pejman Rasti - Université d’Angers\n",
    "David Rousseau - Université d’Angers\n",
    "Martin Weigert - MPI-CBG\n",
    "Uwe Schmidt - MPI-CBG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modified by Tobias Rasse tobias.rasse@mpi-bn.mpg.de for use in OpSeF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "88c8f888-4eb5-438e-8428-0d6f9280aa70",
    "_uuid": "3cf23599bb2587214d3f8b50d3b512bb025159f1"
   },
   "source": [
    "### Importing the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "# h5py to read the data-set\n",
    "import h5py\n",
    "# matplotlob for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# tensorboard\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile as tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import pickle\n",
    "from glob import glob\n",
    "import os\n",
    "from csbdeep.utils import Path, normalize\n",
    "from stardist import fill_label_holes, random_label_cmap, calculate_extents, gputools_available,_draw_polygons\n",
    "from stardist.models import Config2D, StarDist2D, StarDistData2D\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import skimage.segmentation\n",
    "import skimage.morphology\n",
    "import skimage.transform\n",
    "from scipy.ndimage.morphology import binary_dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import disk\n",
    "from skimage.filters import rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_target(label, tMS):\n",
    "    #converts label\n",
    "    size_filter = tMS[\"min_cell_size_label\"]\n",
    "    boundary_width = tMS[\"boundary_width\"]\n",
    "    # next, label the annotations and remove small objects\n",
    "    label = skimage.morphology.label(label)\n",
    "    label = skimage.morphology.remove_small_objects(label, min_size=size_filter)\n",
    "    # next, find boundaries and dilate them\n",
    "    boundaries = skimage.segmentation.find_boundaries(label)\n",
    "    boundaries = binary_dilation(boundaries, iterations=boundary_width)\n",
    "    # we return labels with a single channel, where the values stand for:\n",
    "    # 0: background\n",
    "    # 1: inner cells\n",
    "    # 2: cell boundaries\n",
    "    label_out = np.zeros_like(label, dtype='int64')\n",
    "    label_out[(label != 0) & (boundaries == 0)] = 1\n",
    "    label_out[boundaries == 1] = 2\n",
    "    return label_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(trainModelSettings,which):\n",
    "    '''Loads and pre-processes the data\n",
    "        a list of images is returned'''\n",
    "    # get filenames\n",
    "    XN = sorted(glob('{}/{}/images/*.tif'.format(trainModelSettings[\"path\"],which)))\n",
    "    YN = sorted(glob('{}/{}/masks/*.tif'.format(trainModelSettings[\"path\"],which)))\n",
    "    assert all(Path(x).name==Path(y).name for x,y in zip(XN,YN))\n",
    "    \n",
    "    # load data\n",
    "    X = list(map(tif.imread,XN))\n",
    "    Y = list(map(tif.imread,YN))\n",
    "    n_channel = 1 if X[0].ndim == 2 else X[0].shape[-1]\n",
    "    \n",
    "    # Normalize images and fill small label holes\n",
    "    axis_norm = (0,1)   # normalize channels independently\n",
    "    # axis_norm = (0,1,2) # normalize channels jointly\n",
    "    if n_channel > 1:\n",
    "        print(\"Normalizing image channels %s.\" % ('jointly' if axis_norm is None or 2 in axis_norm else 'independently'))\n",
    "        sys.stdout.flush()\n",
    "    X = [normalize(x,1,99.8,axis=axis_norm) for x in tqdm(X)]\n",
    "    Y = [fill_label_holes(y) for y in tqdm(Y)]\n",
    "    return XN,X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perObjectToBinary(Y,tMS,ch):\n",
    "    ''' converts lavel from one number per object to the core of objects as binary masks'''\n",
    "    Y3 = [label_to_target(ll,tMS) for ll in Y]\n",
    "    Y3binary = [to_categorical(img) for img in Y3]\n",
    "    if ch == 99:\n",
    "        return Y3binary\n",
    "    else:\n",
    "        Y3 = [img[:,:,ch] for img in Y3binary]\n",
    "        Y3 = [img[:,:,None] for img in Y3]\n",
    "        return Y3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The u-net architecture consists of contracting and expansive paths which\n",
    "# shrink and expands the inout image respectivly. \n",
    "# Output image have the same size of input image\n",
    "def get_unet_basic(input_img, trainModelSettings):\n",
    "    # extract variables\n",
    "    activation_out = trainModelSettings[\"unet_activation_out\"]\n",
    "    activation_conv = trainModelSettings[\"unet_activation_conv\"]\n",
    "    n_filters = trainModelSettings[\"n_filters\"]\n",
    "    # contracting path\n",
    "    c1 = conv2d_block(input_img, n_filters*4, activation_conv, kernel_size=3) #The first block of U-net\n",
    "    p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "    c2 = conv2d_block(p1, n_filters*8,activation_conv, kernel_size=3)\n",
    "    p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "    c3 = conv2d_block(p2, n_filters*16,activation_conv, kernel_size=3)\n",
    "    p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "    c4 = conv2d_block(p3, n_filters*32, activation_conv, kernel_size=3)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "    \n",
    "    c5 = conv2d_block(p4, n_filters*64, activation_conv, kernel_size=3)\n",
    "    \n",
    "    # expansive path\n",
    "    u6 = Conv2DTranspose(n_filters*32, (3, 3), strides=(2, 2), padding='same') (c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = conv2d_block(u6, n_filters*32, activation_conv, kernel_size=3)\n",
    "\n",
    "    u7 = Conv2DTranspose(n_filters*16, (3, 3), strides=(2, 2), padding='same') (c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = conv2d_block(u7, n_filters*16, activation_conv, kernel_size=3)\n",
    "\n",
    "    u8 = Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), padding='same') (c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = conv2d_block(u8, n_filters*8, activation_conv, kernel_size=3)\n",
    "\n",
    "    u9 = Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), padding='same') (c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = conv2d_block(u9, n_filters*4, activation_conv, kernel_size=3)\n",
    "    \n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred):\n",
    "    eps = 1e-6\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection) / (K.sum(y_true_f * y_true_f) + K.sum(y_pred_f * y_pred_f) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The u-net architecture consists of contracting and expansive paths which\n",
    "# shrink and expands the inout image respectivly. \n",
    "# Output image have the same size of input image\n",
    "def get_unet_Deep1(input_img, trainModelSettings):\n",
    "    # extract variables\n",
    "    activation_out = trainModelSettings[\"unet_activation_out\"]\n",
    "    activation_conv = trainModelSettings[\"unet_activation_conv\"]\n",
    "    n_filters = trainModelSettings[\"n_filters\"]\n",
    "    # contracting path\n",
    "    c1 = conv2d_block(input_img, n_filters*4, activation_conv, kernel_size=3) #The first block of U-net\n",
    "    p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "    c2 = conv2d_block(p1, n_filters*8,activation_conv, kernel_size=3)\n",
    "    p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "    c3 = conv2d_block(p2, n_filters*16,activation_conv, kernel_size=3)\n",
    "    p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "    c4 = conv2d_block(p3, n_filters*32, activation_conv, kernel_size=3)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "    \n",
    "    c5 = conv2d_block(p4, n_filters*64, activation_conv, kernel_size=3)\n",
    "    p5 = MaxPooling2D(pool_size=(2, 2)) (c5)\n",
    "    \n",
    "    c6 = conv2d_block(p5, n_filters*128, activation_conv, kernel_size=3)\n",
    "    \n",
    "    # expansive path\n",
    "    \n",
    "    u7 = Conv2DTranspose(n_filters*64, (3, 3), strides=(2, 2), padding='same') (c6)\n",
    "    u7 = concatenate([u7, c5])\n",
    "    c7 = conv2d_block(u7, n_filters*64, activation_conv, kernel_size=3)\n",
    "\n",
    "    u8 = Conv2DTranspose(n_filters*32, (3, 3), strides=(2, 2), padding='same') (c7)\n",
    "    u8 = concatenate([u8, c4])\n",
    "    c8 = conv2d_block(u8, n_filters*32, activation_conv, kernel_size=3)\n",
    "\n",
    "    u9 = Conv2DTranspose(n_filters*16, (3, 3), strides=(2, 2), padding='same') (c8)\n",
    "    u9 = concatenate([u9, c3])\n",
    "    c9 = conv2d_block(u9, n_filters*16, activation_conv, kernel_size=3)\n",
    "    \n",
    "    u10 = Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), padding='same') (c9)\n",
    "    u10 = concatenate([u10, c2], axis=3)\n",
    "    c10 = conv2d_block(u10, n_filters*8, activation_conv, kernel_size=3)\n",
    "\n",
    "    u11 = Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), padding='same') (c10)\n",
    "    u11 = concatenate([u11, c1], axis=3)\n",
    "    c11 = conv2d_block(u11, n_filters*4, activation_conv, kernel_size=3)\n",
    "    \n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c11)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each block of u-net architecture consist of two Convolution layers\n",
    "# These two layers are written in a function to make our code clean\n",
    "def conv2d_block(input_tensor, n_filters, av, kernel_size=3):\n",
    "    # first layer\n",
    "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size),\n",
    "               padding=\"same\")(input_tensor)\n",
    "    x = keras.layers.Activation(av)(x)\n",
    "    # second layer\n",
    "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), \n",
    "               padding=\"same\")(x)\n",
    "    x = keras.layers.Activation(av)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = os.path.dirname(os.path.abspath(inspect.stack()[0][1])).replace(\"TrainUNet\",\"Augment\")\n",
    "file_path = \"{}/my_runs/augment_settings_xl.pkl\".format(main_folder)\n",
    "infile = open(file_path,'rb')\n",
    "parameter = pickle.load(infile)\n",
    "print(\"Loading processing pipeline from\",file_path)\n",
    "infile.close()\n",
    "aug_sets,pre_defined_pipelines,data_main_GT,Datasets_Download = parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training settings\n",
    "trainModelSettings = {}\n",
    "trainModelSettings[\"root\"] = data_main_GT\n",
    "trainModelSettings[\"data\"] = \"DSB2018_FL_Nuc_Subset_Basic_Nuc_512All\"\n",
    "trainModelSettings[\"path\"] = os.path.join(trainModelSettings[\"root\"],trainModelSettings[\"data\"])\n",
    "#trainModelSettings[\"model_type\"] = \"UNetBasic\" \n",
    "trainModelSettings[\"model_type\"] = \"UNetDeep1\" # one down and one up-block more than basic\n",
    "trainModelSettings[\"model_name\"] = \"Nuc_Border\"\n",
    "trainModelSettings[\"model_path\"] = \"./models/{}_{}.h5\".format(trainModelSettings[\"model_type\"],trainModelSettings[\"model_name\"])\n",
    "trainModelSettings[\"epochs\"] = 5\n",
    "trainModelSettings[\"unet_activation_out\"]= \"sigmoid\"\n",
    "trainModelSettings[\"unet_activation_conv\"]= \"relu\"\n",
    "trainModelSettings[\"n_filters\"] = 4\n",
    "trainModelSettings[\"label_convert\"] = \"perObjectToCore\"\n",
    "# converts each label in boundary and core an then returns the core\n",
    "# or \"perObjectToBorder\" or \"perObjectToMulti\" (classes as integer)\n",
    "trainModelSettings[\"min_cell_size_label\"] = 25\n",
    "trainModelSettings[\"boundary_width\"] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "XN,X,Y = load_and_preprocess_data(trainModelSettings,\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse label: \n",
    "Keras needs as input one label per class (e.g. background, boundary, cell).\n",
    "We currently have one label per cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first have a look at some images and some label\n",
    "\n",
    "i = 0\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.imshow(X[i][:,:])\n",
    "plt.colorbar()\n",
    "plt.title('Image')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.imshow(Y[i][:,:,0])\n",
    "plt.colorbar()\n",
    "plt.title('Label')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(X[i+5][:,:])\n",
    "plt.colorbar()\n",
    "plt.title('Image')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.imshow(Y[i+5][:,:,0])\n",
    "plt.colorbar()\n",
    "plt.title('Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are \", len(np.unique(Y[0])), \"unique label:\",np.unique(Y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trainModelSettings[\"label_convert\"] == \"perObjectToCore\":\n",
    "    Y3 = perObjectToBinary(Y,trainModelSettings,1) # return core\n",
    "if trainModelSettings[\"label_convert\"] == \"perObjectToBorder\":\n",
    "    Y3 = perObjectToBinary(Y,trainModelSettings,2) # return core\n",
    "if trainModelSettings[\"label_convert\"] == \"perObjectToMulti\":\n",
    "    Y3 = perObjectToBinary(Y,trainModelSettings,99) # return core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please check for a few examples images that the conversion worked well\n",
    "\n",
    "i = 0\n",
    "\n",
    "if trainModelSettings[\"label_convert\"] == \"perObjectToMulti\":\n",
    "    plt.figure(figsize=(15,15))\n",
    "    \n",
    "    plt.subplot(221)\n",
    "    plt.imshow(Y[i][:,:,0])\n",
    "    plt.colorbar()\n",
    "    plt.title('Label old')\n",
    "\n",
    "    plt.subplot(222)\n",
    "    plt.imshow(Y3[i][:,:,0])\n",
    "    plt.colorbar()\n",
    "    plt.title('Label new = 0')\n",
    "\n",
    "    plt.subplot(223)\n",
    "    plt.imshow(Y3[i][:,:,1])\n",
    "    plt.colorbar()\n",
    "    plt.title('Label new = 1')\n",
    "\n",
    "    plt.subplot(224)\n",
    "    plt.imshow(Y3[i][:,:,2])\n",
    "    plt.colorbar()\n",
    "    plt.title('Label new = 2')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    plt.figure(figsize=(15,15))\n",
    "    \n",
    "    plt.subplot(221)\n",
    "    plt.imshow(Y[i][:,:,0])\n",
    "    plt.colorbar()\n",
    "    plt.title('Label old')\n",
    "\n",
    "    plt.subplot(222)\n",
    "    plt.imshow(Y3[i][:,:,0])\n",
    "    plt.colorbar()\n",
    "    plt.title('Label new')\n",
    "\n",
    "    plt.subplot(223)\n",
    "    plt.imshow(Y[i+5][:,:,0])\n",
    "    plt.colorbar()\n",
    "    plt.title('Label old')\n",
    "\n",
    "    plt.subplot(224)\n",
    "    plt.imshow(Y3[i+5][:,:,0])\n",
    "    plt.colorbar()\n",
    "    plt.title('Label new')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old function for non-pre-augmented data, keep!\n",
    "# Split into train and validation datasets.\n",
    "assert len(X) > 1, \"not enough training data\"\n",
    "rng = np.random.RandomState(42)\n",
    "ind = rng.permutation(len(X))\n",
    "n_val = max(1, int(round(0.15 * len(ind))))\n",
    "ind_train, ind_val = ind[:-n_val], ind[-n_val:]\n",
    "X_val, Y_val = [X[i] for i in ind_val]  , [Y3[i] for i in ind_val]\n",
    "X_trn, Y_trn = [X[i] for i in ind_train], [Y3[i] for i in ind_train] \n",
    "print('number of images: %3d' % len(X))\n",
    "print('- training:       %3d' % len(X_trn))\n",
    "print('- validation:     %3d' % len(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get core part of name (that defines orginal image BEFORE augmentation)\n",
    "get_names = list(set([os.path.split(fn)[1].split(\"_\")[0] for fn in XN]))\n",
    "assert len(X) > 1, \"not enough training data\"\n",
    "# create random subset\n",
    "rng = np.random.RandomState(42)\n",
    "ind = rng.permutation(len(get_names))\n",
    "n_val = max(1, int(round(0.15 * len(ind))))\n",
    "ind_train, ind_val = ind[:-n_val], ind[-n_val:]\n",
    "val_names, trn_names = [get_names[i] for i in ind_val],[get_names[i] for i in ind_train]\n",
    "# map subset back on orginal images\n",
    "val_img_index = [i for i in range(0,len(XN)) if os.path.split(XN[i])[1].split(\"_\")[0] in val_names]\n",
    "trn_img_index = [i for i in range(0,len(XN)) if os.path.split(XN[i])[1].split(\"_\")[0] in trn_names]\n",
    "X_val, Y_val = [X[i] for i in val_img_index]  , [Y3[i] for i in val_img_index]\n",
    "X_trn, Y_trn = [X[i] for i in trn_img_index], [Y3[i] for i in trn_img_index]\n",
    "# print results\n",
    "print('number of images: %3d' % len(X))\n",
    "print('- training:       %3d' % len(X_trn))\n",
    "print('- validation:     %3d' % len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(XN,X,Y):\n",
    "    # splits data in training and validation dataset\n",
    "    # get core part of name (that defines orginal image BEFORE augmentation)\n",
    "    get_names = list(set([os.path.split(fn)[1].split(\"_\")[0] for fn in XN]))\n",
    "    assert len(X) > 1, \"not enough training data\"\n",
    "    # create random subset\n",
    "    rng = np.random.RandomState(42)\n",
    "    ind = rng.permutation(len(get_names))\n",
    "    n_val = max(1, int(round(0.15 * len(ind))))\n",
    "    ind_train, ind_val = ind[:-n_val], ind[-n_val:]\n",
    "    val_names, trn_names = [get_names[i] for i in ind_val],[get_names[i] for i in ind_train]\n",
    "    # map subset back on orginal images\n",
    "    val_img_index = [i for i in range(0,len(XN)) if os.path.split(XN[i])[1].split(\"_\")[0] in val_names]\n",
    "    trn_img_index = [i for i in range(0,len(XN)) if os.path.split(XN[i])[1].split(\"_\")[0] in trn_names]\n",
    "    X_val, Y_val = [X[i] for i in val_img_index]  , [Y3[i] for i in val_img_index]\n",
    "    X_trn, Y_trn = [X[i] for i in trn_img_index], [Y3[i] for i in trn_img_index]\n",
    "    # print results\n",
    "    print('number of images: %3d' % len(X))\n",
    "    print('- training:       %3d' % len(X_trn))\n",
    "    print('- validation:     %3d' % len(X_val))\n",
    "    return X_val, Y_val,X_trn, Y_trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val,X_trn, Y_trn = split_train_val(XN,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add axis were needed and stack images\n",
    "\n",
    "# train\n",
    "X_trn_arr = np.stack([img[:,:,None] for img in X_trn])\n",
    "print(X_trn_arr.shape)\n",
    "Y_trn_arr = np.stack([img for img in Y_trn])\n",
    "print(Y_trn_arr.shape)\n",
    "\n",
    "# validate\n",
    "X_val_arr = np.stack([img[:,:,None] for img in X_val])\n",
    "print(X_val_arr.shape)\n",
    "Y_val_arr = np.stack([img for img in Y_val])\n",
    "print(Y_val_arr.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input((X_trn_arr.shape[1], X_trn_arr.shape[2], 1), name='img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and Compiling the model\n",
    "if trainModelSettings[\"model_type\"] == \"UNetBasic\":\n",
    "    model = get_unet_basic(input_img,trainModelSettings)\n",
    "    model.compile(optimizer=Adam(), loss=\"binary_crossentropy\", metrics=[dice_coefficient])\n",
    "    model.summary()\n",
    "    \n",
    "if trainModelSettings[\"model_type\"] == \"UNetDeep1\":\n",
    "    model = get_unet_Deep1(input_img,trainModelSettings)\n",
    "    model.compile(optimizer=Adam(), loss=\"binary_crossentropy\", metrics=[dice_coefficient])\n",
    "    model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the log and show it by tensorboard\n",
    "NAME= trainModelSettings[\"model_type\"]\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiting the model \n",
    "results = model.fit(X_trn_arr, Y_trn_arr, batch_size=1, epochs=trainModelSettings[\"epochs\"], callbacks=[tensorboard], validation_data=(X_val_arr, Y_val_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(trainModelSettings[\"model_path\"])\n",
    "print(\"Saving model to \",trainModelSettings[\"model_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "XN_test,X_test,Y_test = load_and_preprocess_data(trainModelSettings,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_arr = np.stack([img[:,:,None] for img in X_test])\n",
    "Y_trn_arr = np.stack([img for img in Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = model.predict(np.stack(X_test_arr), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold predictions\n",
    "preds_test_t = (preds_test > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.subplot(331)  #sublot(Anzahl Zeilen Anzahl Spalten Bild Nummer)\\\n",
    "plt.imshow(X_test[i][:,:])\n",
    "plt.colorbar()\n",
    "plt.title('Input')\n",
    "\n",
    "plt.subplot(332)  #sublot(Anzahl Zeilen Anzahl Spalten Bild Nummer)\\\n",
    "plt.imshow(Y_test[i][:,:])\n",
    "plt.colorbar()\n",
    "plt.title('Label')\n",
    "\n",
    "plt.subplot(333)  #sublot(Anzahl Zeilen Anzahl Spalten Bild Nummer)\\\n",
    "plt.imshow(preds_test_t[i][:,:,0])\n",
    "#plt.imshow(preds_train[i][:,:,0])\n",
    "plt.colorbar()\n",
    "plt.title('Prediction')\n",
    "\n",
    "plt.subplot(334)  #sublot(Anzahl Zeilen Anzahl Spalten Bild Nummer)\\\n",
    "plt.imshow(X_test[i+1][:,:])\n",
    "plt.colorbar()\n",
    "plt.title('Input')\n",
    "\n",
    "plt.subplot(335)  #sublot(Anzahl Zeilen Anzahl Spalten Bild Nummer)\\\n",
    "plt.imshow(Y_test[i+1][:,:])\n",
    "plt.colorbar()\n",
    "plt.title('Label')\n",
    "\n",
    "plt.subplot(336)  #sublot(Anzahl Zeilen Anzahl Spalten Bild Nummer)\\\n",
    "plt.imshow(preds_test_t[i+1][:,:,0])\n",
    "#plt.imshow(preds_train[i][:,:,0])\n",
    "plt.colorbar()\n",
    "plt.title('Prediction')\n",
    "\n",
    "plt.subplot(337)  #sublot(Anzahl Zeilen Anzahl Spalten Bild Nummer)\\\n",
    "plt.imshow(X_test[i+2][:,:])\n",
    "plt.colorbar()\n",
    "plt.title('Input')\n",
    "\n",
    "plt.subplot(338)  #sublot(Anzahl Zeilen Anzahl Spalten Bild Nummer)\\\n",
    "plt.imshow(Y_test[i+2][:,:])\n",
    "plt.colorbar()\n",
    "plt.title('Label')\n",
    "\n",
    "plt.subplot(339)  #sublot(Anzahl Zeilen Anzahl Spalten Bild Nummer)\\\n",
    "plt.imshow(preds_test_t[i+2][:,:,0])\n",
    "#plt.imshow(preds_train[i][:,:,0])\n",
    "plt.colorbar()\n",
    "plt.title('Prediction')\n",
    "\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Until here we have only pixel wise predictions\n",
    "# These need to be converted in segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = list(range(20))\n",
    "print(ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_nums():\n",
    "    n = 0\n",
    "    while n < 4:\n",
    "        yield n\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = gen_nums()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in nums:\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_nums_from_list(ml):\n",
    "    for n in range(len(ml)):\n",
    "        yield ml[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = gen_nums_from_list(ml)\n",
    "print(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in nums:\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> nums = gen_nums()\n",
    ">>> type(nums)\n",
    "<class 'generator'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processing_prm = {}\n",
    "post_processing_prm[\"prediction_threshold\"] = 0.5 # defines probability threshold \n",
    "post_processing_prm[\"smooth_distance\"] = 3 # Selem Radius used to smooth distance map\n",
    "post_processing_prm[\"min_distance\"] = 20 # minimal distance of local maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_to_label(pred,ppp):\n",
    "    \" does atershed segmentation based on cell predictions\"\n",
    "    # define generator\n",
    "    def load_img(pred):\n",
    "        for i in range(len(pred)):\n",
    "            yield pred[0][:,:,0]\n",
    "    prob = load_img(pred)\n",
    "    seg_masks = []\n",
    "    for p in prob:\n",
    "        # probability to binary\n",
    "        cells = p > ppp[\"prediction_threshold\"]\n",
    "        # create label\n",
    "        label_cells = skimage.morphology.label(cells)\n",
    "        # create distance map & smooth\n",
    "        distance = distance_transform_edt(cells)\n",
    "        selem = disk(ppp[\"smooth_distance\"])\n",
    "        distance_smooth = skimage.filters.median(distance, selem=selem)\n",
    "        # get local maxima\n",
    "        local_maxima = skimage.feature.peak_local_max(distance_smooth, indices=False, \n",
    "                                                      min_distance = ppp[\"min_distance\"], labels = label_cells)\n",
    "        seeds = skimage.morphology.label(local_maxima)\n",
    "        labels = skimage.morphology.watershed(-distance_smooth, seeds, mask=cells)\n",
    "        seg_masks.append(labels)\n",
    "    return seg_masks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = prediction_to_label(preds_test,post_processing_prm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_img = preds_test[0][:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_mask = pred_img < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = pred_img > 0.5\n",
    "label_cells = skimage.morphology.label(cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = distance_transform_edt(cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selem = disk(3)\n",
    "#distance_smooth = rank.mean_percentile(distance, selem=selem, p0=0.05, p1=0.95)\n",
    "distance_smooth = skimage.filters.median(distance, selem=selem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_ws_simple = segment_watershed_simple(pred_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_maxima = skimage.feature.peak_local_max(distance_smooth, indices=False, min_distance = 20, labels = label_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage as ndi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = ndi.label(local_maxima)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = skimage.morphology.watershed(-distance_smooth, markers, mask=cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "#plt.imshow(labels)\n",
    "plt.imshow(distance_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "#plt.imshow(labels)\n",
    "plt.imshow(sm[0])\n",
    "plt.colorbar()\n",
    "#plt.imshow(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local_maxima = skimage.feature.peak_local_max(distance, indices=False, min_distance = 10, labels = label_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_maxima = skimage.feature.peak_local_max(distance, indices=False, labels = label_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = skimage.morphology.label(local_maxima)\n",
    "# run watersehd with inverted distance transform as height map and background mask\n",
    "#label = skimage.morphology.watershed(distance, seeds, mask=cells)\n",
    "label = skimage.morphology.watershed(distance,seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(131)  #sublot(Anzahl Zeilen Anzahl Spalten Bild Nummer)\\\n",
    "plt.imshow(local_maxima)\n",
    "#plt.colorbar()\n",
    "plt.title('Predictions')\n",
    "\n",
    "plt.subplot(132)  #sublot(Anzahl Zeilen Anzahl Spalten Bild Nummer)\\\n",
    "plt.imshow(label)\n",
    "plt.colorbar()\n",
    "plt.title('Segmentation')\n",
    "\n",
    "plt.subplot(133)  #sublot(Anzahl Zeilen Anzahl Spalten Bild Nummer)\\\n",
    "plt.imshow(distance)\n",
    "#plt.colorbar()\n",
    "plt.title('Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_watershed_simple(prediction, cell_threshold=0.5, min_cell_size=25):\n",
    "    \n",
    "    # we use a background mask to restrict the watershed segmentation\n",
    "    # to nuclei \n",
    "    background_mask = prediction < cell_threshold\n",
    "    \n",
    "    # use distance transform of thresholded boundary predictions as height map\n",
    "    # note that the scipy distance transform computes the distance to the background\n",
    "    cells = prediction > cell_threshold\n",
    "    distance = distance_transform_edt(cells)\n",
    "    # use local maxima of the distance transform as watershed seeds\n",
    "    local_maxima = skimage.feature.peak_local_max(distance, indices=False, footprint=np.ones((3, 3)),\n",
    "                                                  labels=background_mask)\n",
    "    seeds = skimage.morphology.label(local_maxima)\n",
    "    # run watersehd with inverted distance transform as height map and background mask\n",
    "    label = skimage.morphology.watershed(-distance, seeds, mask=background_mask)\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional imports for watershed\n",
    "import skimage.feature\n",
    "from scipy.ndimage.morphology import distance_transform_edt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_cc(prediction, min_cell_size=25):\n",
    "    cell = prediction[1] > .5\n",
    "    label = skimage.morphology.label(cell)\n",
    "    label = skimage.morphology.remove_small_objects(label,\n",
    "                                                     min_size=min_cell_size)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_watershed(prediction, boundary_threshold=.5, \n",
    "                      background_threshold=.5, min_cell_size=25):\n",
    "    background = prediction[0]\n",
    "    boundaries = prediction[-1]\n",
    "    \n",
    "    # we use a background mask to restrict the watershed segmentation\n",
    "    # to nuclei and boundaries\n",
    "    background_mask = background < background_threshold\n",
    "    \n",
    "    # use distance transform of thresholded boundary predictions as height map\n",
    "    # note that the scipy distance transform computes the distance to the background\n",
    "    thresholded = boundaries < boundary_threshold\n",
    "    distance = distance_transform_edt(thresholded)\n",
    "    # use local maxima of the distance transform as watershed seeds\n",
    "    local_maxima = skimage.feature.peak_local_max(distance, indices=False, footprint=np.ones((3, 3)),\n",
    "                                                  labels=background_mask)\n",
    "    seeds = skimage.morphology.label(local_maxima)\n",
    "    # run watersehd with inverted distance transform as height map and background mask\n",
    "    label = skimage.morphology.watershed(-distance, seeds, mask=background_mask)\n",
    "    \n",
    "    return label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
