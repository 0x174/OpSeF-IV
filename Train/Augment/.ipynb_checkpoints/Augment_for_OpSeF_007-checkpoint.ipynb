{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## universal segmentor, run notebook before training\n",
    "## Limitations:\n",
    "## All input images must have the same size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import tifffile as tif\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticdeform\n",
    "import albumentations\n",
    "import Augmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from imgaug.augmentables.segmaps import SegmentationMapsOnImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csbdeep.utils import Path, download_and_extract_zip_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from zipfile import ZipFile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.morphology\n",
    "import skimage.transform\n",
    "from scipy.ndimage.morphology import binary_dilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_split(label, augS):\n",
    "    #creates gap between objects\n",
    "    boundary_width = augS[\"boundary_width\"]\n",
    "    base_mask = np.zeros((label.shape), dtype = np.uint8)\n",
    "    # itter over all label\n",
    "    for l in np.unique(label):\n",
    "        if l > 0:\n",
    "            mask_cell = np.zeros((label.shape), dtype = bool)\n",
    "            mask_cell[np.where(label == l)] = 1\n",
    "            mask_cell = binary_dilation(mask_cell, iterations=boundary_width)\n",
    "            base_mask = base_mask + mask_cell\n",
    "            split= np.where(base_mask > 1) # covered by more than on object\n",
    "    label[split] = 0\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_batch(im,ma,names,aug_settings,silent = True):\n",
    "    '''creates random augmentations\n",
    "       based on predefined sets handed over\n",
    "       aug_per_img defines the number of different augmentation tpyes per image\n",
    "       itter the augmentation per image per pipeline\n",
    "       crops and enlarges to avoid egde effects\n",
    "    '''\n",
    "    # renumber pipelines for augmentation\n",
    "    crop = aug_settings[\"crop\"]\n",
    "    my_pipelines = {}\n",
    "    for i,key in enumerate(aug_settings[\"pre_defined_pipeline\"]):\n",
    "        my_pipelines[i] = aug_settings[\"aug_sets\"][key]\n",
    "    # extract variables\n",
    "    aug_per_img = aug_settings[\"aug_per_img\"]\n",
    "    itter = aug_settings[\"itter\"]\n",
    "    border= aug_settings[\"border\"]\n",
    "    clean_it = aug_settings[\"clean_it\"]\n",
    "    # create lists\n",
    "    images_aug = []\n",
    "    masks_aug = []\n",
    "    names_aug = []\n",
    "    for i,image in enumerate(im):\n",
    "        mask = ma[i,:,:,:]\n",
    "        segmap = SegmentationMapsOnImage(mask, shape=image.shape)\n",
    "        AugList = random.sample(range(0,len(my_pipelines)),aug_per_img)\n",
    "        for aug_id in AugList:\n",
    "            pipeline = my_pipelines[aug_id]\n",
    "            if not silent:\n",
    "                print(aug_id)\n",
    "                print(pipeline)\n",
    "            for it in range(0,itter):\n",
    "                images_aug_i, segmaps_aug_i = pipeline(image=image, segmentation_maps=segmap)\n",
    "                if clean_it:\n",
    "                    im_new,mask_new = clean_border(images_aug_i,segmaps_aug_i.get_arr(),border)\n",
    "                    if sum(crop) > 0:\n",
    "                        im_new,mask_new = crop_image(im_new,mask_new,crop)\n",
    "                    images_aug.append(im_new)\n",
    "                    masks_aug.append(mask_new)\n",
    "                else:\n",
    "                    if sum(crop) > 0: # crop only\n",
    "                        im_new,mask_new = crop_image(images_aug_i,segmaps_aug_i.get_arr(),crop)\n",
    "                        images_aug.append(im_new)\n",
    "                        masks_aug.append(mask_new)\n",
    "                    else: # just copy    \n",
    "                        images_aug.append(images_aug_i)\n",
    "                        masks_aug.append(segmaps_aug_i.get_arr())\n",
    "                names_aug.append(names[i].replace(\".tif\",\"_{}_{}.tif\".format(str(aug_id).zfill(4),str(it).zfill(3))))\n",
    "    return images_aug,masks_aug,names_aug    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(IMG_path,MASK_path,start_id,batch_size,augS,segmaps_per_img = 1):\n",
    "    # loads first image\n",
    "    image = imageio.imread(IMG_path[0])\n",
    "    mask = imageio.imread(MASK_path[0])\n",
    "    im_names = []\n",
    "    img_batch = np.zeros((batch_size,*image.shape),dtype = image.dtype)\n",
    "    mask_batch = np.zeros((batch_size,*mask.shape,segmaps_per_img),dtype = mask.dtype)\n",
    "    for i in range(0,batch_size):\n",
    "        img_batch[i,:,:] = imageio.imread(IMG_path[start_id+i])\n",
    "        mask = imageio.imread(MASK_path[start_id+i])\n",
    "        if augS[\"add_gap\"]: # adds a gap\n",
    "            mask = label_split(mask, augS)\n",
    "        mask_batch[i,:,:,0] = mask\n",
    "        im_names.append(os.path.split(IMG_path[start_id+i])[1])\n",
    "    return img_batch,mask_batch,im_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_batch(img_aug,mask_aug,names_new,path):\n",
    "    # saves a batch of augmented images to the new directory\n",
    "    for i, im in enumerate(img_aug):\n",
    "        im_name = os.path.join(path,\"images\",names_new[i])\n",
    "        #print(\"Saving im_name..\", im_name)\n",
    "        imageio.imwrite(im_name,im)\n",
    "        mask_name = os.path.join(path,\"masks\",names_new[i])\n",
    "        imageio.imwrite(mask_name,mask_aug[i])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_border(im,ma,border):\n",
    "    # enlarges and crops to avoid egde effects\n",
    "    # get sizes\n",
    "    x,y = im.shape\n",
    "    cropx = x - (2*border)\n",
    "    cropy = y - (2*border)\n",
    "    # define pipeline\n",
    "    normP = iaa.Sequential([\n",
    "        iaa.CropToFixedSize(width = cropx ,height = cropy, position = \"center\"),     # crop\n",
    "        iaa.Resize({\"width\" : x ,\"height\" : y})\n",
    "        ], random_order=False)\n",
    "    # create segmap\n",
    "    segmap_norm = SegmentationMapsOnImage(ma, shape=im.shape)\n",
    "    images_aug, segmaps_aug = normP(image=im, segmentation_maps=segmap_norm)\n",
    "    masks_aug = (segmaps_aug.get_arr())\n",
    "    return images_aug,masks_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(im,ma,crop_dims):\n",
    "    # crops image\n",
    "    cropy, cropx = crop_dims\n",
    "    # define pipeline\n",
    "    normP = iaa.Sequential([\n",
    "        iaa.CropToFixedSize(width = cropx ,height = cropy, position = \"center\")], random_order=False)\n",
    "    # create segmap\n",
    "    segmap_norm = SegmentationMapsOnImage(ma, shape=im.shape)\n",
    "    images_aug, segmaps_aug = normP(image=im, segmentation_maps=segmap_norm)\n",
    "    masks_aug = (segmaps_aug.get_arr())\n",
    "    return images_aug,masks_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folder(tms,new_name,crop):\n",
    "    #creates new folder &\n",
    "    # copies Train and Validate Datasets\n",
    "    new_folder = os.path.join(trainModelSettings[\"root\"],new_name)\n",
    "    if not os.path.exists(new_folder):\n",
    "        os.makedirs(new_folder)\n",
    "        os.makedirs(os.path.join(new_folder,\"train\",\"images\"))\n",
    "        os.makedirs(os.path.join(new_folder,\"train\",\"masks\"))\n",
    "        # copy test & validate data (will not be augmented)\n",
    "        for copy_only in [\"test\"]:\n",
    "            if sum(crop) > 0: # means crop image, just make folder and crop later\n",
    "                os.makedirs(os.path.join(new_folder,\"test\",\"images\"))\n",
    "                os.makedirs(os.path.join(new_folder,\"test\",\"masks\"))\n",
    "            else:\n",
    "                src = os.path.join(tms[\"path\"],copy_only)\n",
    "                dst = os.path.join(trainModelSettings[\"root\"],new_name,copy_only)\n",
    "                if os.path.exists(src):\n",
    "                    print(\"Copy \",src, \" to \", dst)\n",
    "                    shutil.copytree(src,dst)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_owncloud(url, outfile=None):\n",
    "    url = url.replace('?dl=0', '')\n",
    "    url = url.replace('?dl=1', '')\n",
    "    if not '?raw=1' in url:\n",
    "        url += \"?raw=1\"\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    if not outfile:\n",
    "        outfile = extract_filename_from_url(url)\n",
    "    if outfile:\n",
    "        print(\"Downloading...\",outfile)\n",
    "        open(outfile, 'wb').write(r.content)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_file(outfile):\n",
    "    # unzips downloaded file\n",
    "    #targetdir = os.path.join(os.path.split(outfile)[0],os.path.split(outfile)[1].replace(\".zip\",\"\"))\n",
    "    targetdir = os.path.join(os.path.split(outfile)[0])\n",
    "    with ZipFile(outfile,'r') as zip_file:\n",
    "        print(' extracting to',targetdir)\n",
    "        zip_file.extractall(str(targetdir))\n",
    "        provided = zip_file.namelist()\n",
    "    print(\"Done\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(DD,load_me,main_root,keep_zip=False):\n",
    "    # downloads and extracts data from owncloud\n",
    "    # define url\n",
    "    url = DD[load_me]\n",
    "    # make folder\n",
    "    new_folder = os.path.join(main_root)\n",
    "    if not os.path.exists('my_folder'):\n",
    "        os.makedirs('my_folder')\n",
    "    print(\"Make folder\")\n",
    "    # download\n",
    "    outfile = os.path.join(new_folder,\"{}.zip\".format(load_me))\n",
    "    print(\"Start download\")\n",
    "    download_from_owncloud(url,outfile)\n",
    "    # unzip\n",
    "    print(\"Unzip\")\n",
    "    unzip_file(outfile)\n",
    "    # cleanup\n",
    "    if not keep_zip:\n",
    "        print(\"Deleting..\",outfile)\n",
    "        os.remove(outfile)\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processing pipeline from ./my_runs/augment_settings_xl.pkl\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./my_runs/augment_settings_xl.pkl\"\n",
    "infile = open(file_path,'rb')\n",
    "parameter = pickle.load(infile)\n",
    "print(\"Loading processing pipeline from\",file_path)\n",
    "infile.close()\n",
    "aug_sets,pre_defined_pipelines,data_main_GT,Datasets_Download = parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define init download (if needed)\n",
    "trainModelSettings = {}\n",
    "trainModelSettings[\"root\"] = data_main_GT\n",
    "trainModelSettings[\"data\"] = \"DSB2018_FL_Nuc_Subset\"\n",
    "trainModelSettings[\"Download_Data\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make folder\n",
      "Start download\n",
      "Downloading... /mnt/ag-microscopy/SampleDataML/OpSeF_XL_Data/GT/DSB2018_FL_Nuc_Subset.zip\n",
      "Unzip\n",
      " extracting to /mnt/ag-microscopy/SampleDataML/OpSeF_XL_Data/GT\n",
      "Done\n",
      "Deleting.. /mnt/ag-microscopy/SampleDataML/OpSeF_XL_Data/GT/DSB2018_FL_Nuc_Subset.zip\n"
     ]
    }
   ],
   "source": [
    "if trainModelSettings[\"Download_Data\"]:\n",
    "    download_data(Datasets_Download,trainModelSettings[\"data\"],trainModelSettings[\"root\"],False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define how to augment (chance each time for a pipeline)\n",
    "augment_setting = {}\n",
    "augment_setting[\"Aug_Pipeline_Name\"] = \"Basic_Nuc\" # see full documentation in OpSeF_XL_Configure_002\n",
    "augment_setting[\"Tag\"] = \"512AllGap\"\n",
    "augment_setting[\"new name\"] = \"{}_{}_{}\".format(trainModelSettings[\"data\"],augment_setting[\"Aug_Pipeline_Name\"],augment_setting[\"Tag\"])\n",
    "augment_setting[\"aug_sets\"] =  aug_sets\n",
    "augment_setting[\"pre_defined_pipeline\"] = pre_defined_pipelines[augment_setting[\"Aug_Pipeline_Name\"]]\n",
    "# currently available sets: Basic_Nuc, Light_Nuc,Heavy_Nuc,Versatile_Nuc\n",
    "augment_setting[\"batch_size\"] = 10 # this number of images are one batch\n",
    "augment_setting[\"itter\"] = 3 # the augment pipeline will be applied itter times per image\n",
    "augment_setting[\"aug_per_img\"] = 3 # this number of pipelines will be run for each image in the batch\n",
    "augment_setting[\"border\"] = 50\n",
    "augment_setting[\"clean_it\"] = True\n",
    "augment_setting[\"crop\"] = [512,512] # enter [0] or [0,0] to skip cropping\n",
    "augment_setting[\"add_gap\"] = True # makes gap between cells\n",
    "augment_setting[\"boundary_width\"] = 2 # define gap width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define location of data\n",
    "trainModelSettings[\"path\"] = os.path.join(trainModelSettings[\"root\"],trainModelSettings[\"data\"])\n",
    "trainModelSettings[\"path_out\"] =  os.path.join(trainModelSettings[\"root\"],augment_setting[\"new name\"],\"train\")\n",
    "trainModelSettings[\"path_out_test\"] = os.path.join(trainModelSettings[\"root\"],augment_setting[\"new name\"],\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters for augmentation were stored in this file: \n",
      " ./Train/Augment/my_runs/Parameter_Augment_DSB2018_FL_Nuc_Subset_Basic_Nuc_512AllGap.pkl\n"
     ]
    }
   ],
   "source": [
    "#  save settings\n",
    "parameter = [augment_setting,trainModelSettings]\n",
    "\n",
    "# save it\n",
    "file_name = \"./my_runs/Parameter_Augment_{}.pkl\".format(augment_setting[\"new name\"])\n",
    "file_name_load = \"./Train/Augment/my_runs/Parameter_Augment_{}.pkl\".format(augment_setting[\"new name\"])\n",
    "print(\"The parameters for augmentation were stored in this file: \\n\",file_name_load)\n",
    "outfile = open(file_name,'wb')\n",
    "pickle.dump(parameter,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and analyse input\n",
    "IMG_path = sorted(glob('{}/train/images/*.tif'.format(trainModelSettings[\"path\"])))\n",
    "MASK_path = sorted(glob('{}/train/masks/*.tif'.format(trainModelSettings[\"path\"])))\n",
    "assert all(os.path.split(IMG_path[x])[1]==os.path.split(MASK_path[x])[1] for x in range(0,len(IMG_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make folder & copy test images that will not be augmented\n",
    "make_folder(trainModelSettings,augment_setting[\"new name\"],augment_setting[\"crop\"])\n",
    "start_id = 0\n",
    "batch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Batch:  0\n",
      "Running Batch:  1\n",
      "Running Batch:  2\n",
      "Running Batch:  3\n",
      "Running Batch:  4\n",
      "Running Batch:  5\n",
      "Running Batch:  6\n",
      "Running Batch:  7\n",
      "Running the last images:  7\n"
     ]
    }
   ],
   "source": [
    "# run all images in batches\n",
    "while start_id < len(IMG_path):\n",
    "    print(\"Running Batch: \", batch)\n",
    "    # load images:\n",
    "    if (start_id + augment_setting[\"batch_size\"]) < len(IMG_path):\n",
    "        X,Y,N = load_batch(IMG_path,MASK_path,start_id,augment_setting[\"batch_size\"],augment_setting)\n",
    "    else:\n",
    "        rest = len(IMG_path) - start_id\n",
    "        print(\"Running the last images: \",rest)\n",
    "        X,Y,N = load_batch(IMG_path,MASK_path,start_id,rest,augment_setting)\n",
    "    Xnew,Ynew,Nnew = augment_batch(X,Y,N,augment_setting)\n",
    "    save_batch(Xnew,Ynew,Nnew,trainModelSettings[\"path_out\"])\n",
    "    start_id += augment_setting[\"batch_size\"]\n",
    "    batch += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sum(augment_setting[\"crop\"]) > 0:\n",
    "    # load test images\n",
    "    IMG_path = sorted(glob('{}/test/images/*.tif'.format(trainModelSettings[\"path\"])))\n",
    "    MASK_path = sorted(glob('{}/test/masks/*.tif'.format(trainModelSettings[\"path\"])))\n",
    "    assert all(os.path.split(IMG_path[x])[1]==os.path.split(MASK_path[x])[1] for x in range(0,len(IMG_path)))\n",
    "    #\n",
    "    for img,mask in zip(IMG_path,MASK_path):\n",
    "        #print(img)\n",
    "        im_new,mask_new = crop_image(imageio.imread(img),imageio.imread(mask),augment_setting[\"crop\"])\n",
    "        #print(im_new.shape)\n",
    "        im_name = os.path.join(trainModelSettings[\"path_out_test\"],\"images\",os.path.split(img)[1])\n",
    "        mask_name = os.path.join(trainModelSettings[\"path_out_test\"],\"masks\",os.path.split(img)[1])\n",
    "        #print(im_name)\n",
    "        imageio.imwrite(im_name,im_new)\n",
    "        imageio.imwrite(mask_name,mask_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
