{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements:\n",
    "\n",
    "Tested with opsef003.yml (see attached file)\n",
    "opsef002 + n2v = opsef003\n",
    "\n",
    "on a GeForce RTX 2080 with 8GB RAM\n",
    "on ubuntu/18.04.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on OpSeF_IV_Run_002_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adaped from:\n",
    "\n",
    "https://github.com/MouseLand/cellpose\n",
    "\n",
    "https://github.com/CellProfiler/CellProfiler\n",
    "\n",
    "https://github.com/mpicbg-csbd/stardist\n",
    "\n",
    "https://github.com/scikit-image/scikit-image\n",
    "\n",
    "https://github.com/VolkerH/unet-nuclei/\n",
    "\n",
    "Thanks to:\n",
    "\n",
    "All developer of the above mentioned repositories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic libs\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import inspect\n",
    "from glob import glob\n",
    "\n",
    "import tifffile as tif\n",
    "\n",
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections\n",
    "import math\n",
    "\n",
    "import pickle\n",
    "\n",
    "import networkx as nx\n",
    "%matplotlib inline\n",
    "\n",
    "# skimage\n",
    "import skimage\n",
    "from skimage import transform, io, filters, measure, morphology,img_as_float  \n",
    "from skimage.color import label2rgb,gray2rgb\n",
    "from skimage.filters import gaussian, rank, threshold_otsu\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.measure import label, regionprops, regionprops_table\n",
    "from skimage.morphology import disk, watershed\n",
    "\n",
    "# scipy\n",
    "from scipy.signal import medfilt\n",
    "from scipy.ndimage import generate_binary_structure, binary_dilation\n",
    "\n",
    "# for cluster analysis\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/trasse/anaconda3/envs/opsef003/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/trasse/anaconda3/envs/opsef003/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/trasse/anaconda3/envs/opsef003/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/trasse/anaconda3/envs/opsef003/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/trasse/anaconda3/envs/opsef003/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/trasse/anaconda3/envs/opsef003/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "main_folder = os.path.dirname(os.path.abspath(inspect.stack()[0][1]))\n",
    "import_path = os.path.join(main_folder,\"Utils_and_Configs\")\n",
    "if import_path not in sys.path:\n",
    "    sys.path.append(import_path)\n",
    "\n",
    "# import from import_path\n",
    "from Tools_002 import *\n",
    "from UNet_CP01 import *\n",
    "from Segmentation_Func_06 import *\n",
    "from Pre_Post_Process002 import *\n",
    "from N2V_DataGeneratorTR001 import *\n",
    "from opsef_core_002 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitpath(path, maxdepth=20):\n",
    "    '''Splits a path in all its parts'''\n",
    "    ( head, tail ) = os.path.split(path)\n",
    "    return splitpath(head, maxdepth - 1) + [ tail ] \\\n",
    "        if maxdepth and head and head != path \\\n",
    "        else [ head or tail ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_mask_to_img_dics(fp):\n",
    "    ''' Imports filepair list as dic of dics'''\n",
    "    # read content\n",
    "    with open(pair_dic_fn) as f:\n",
    "        content = f.readlines()\n",
    "    # remove whitespace characters like `\\n` at the end of each line\n",
    "    content = [x.strip() for x in content]\n",
    "    # make target dic\n",
    "    mask_to_img_dic_dic = {}\n",
    "    # create static part of filepath\n",
    "    path_as_list = splitpath(fp)\n",
    "    folder_base = os.path.join(*path_as_list[:-2])\n",
    "    # convert content\n",
    "    for seg_mask_id in range(1,len(content[0].split(\";\"))):\n",
    "        mask_to_img_dic = {}\n",
    "        for line in content:\n",
    "            mylist = line.split(\";\")\n",
    "            mask_to_img_dic[folder_base+mylist[0]] = folder_base + mylist[seg_mask_id]\n",
    "        mask_to_img_dic_dic[seg_mask_id] = mask_to_img_dic\n",
    "    return mask_to_img_dic_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load parameter\n",
    "the parameter for processing need to be defined in the notebook.\n",
    "Opsef_Setup_000X\n",
    "this notebook will print in the end a file_path.\n",
    "Please cut and paste it below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processing pipeline from /mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Parameter_SDB2018Cellsinv_Run_005.pkl\n"
     ]
    }
   ],
   "source": [
    "# load the info on the original segmentation\n",
    "\n",
    "file_path = \"/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Parameter_SDB2018Cellsinv_Run_005.pkl\"\n",
    "infile = open(file_path,'rb')\n",
    "parameter = pickle.load(infile)\n",
    "print(\"Loading processing pipeline from\",file_path)\n",
    "infile.close()\n",
    "pc,input_def,run_def,initModelSettings = parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/00_InputRaw/8bitSum_00419.tif': '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/11_SegMasksFromFiji/000_SD_Mask_0_RGBInput_000_00419.tif',\n",
       "  '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/00_InputRaw/8bitSum_00173.tif': '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/11_SegMasksFromFiji/000_SD_Mask_0_RGBInput_000_00173.tif',\n",
       "  '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/00_InputRaw/8bitSum_00066.tif': '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/11_SegMasksFromFiji/000_SD_Mask_0_RGBInput_000_00066.tif',\n",
       "  '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/00_InputRaw/8bitSum_00125.tif': '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/11_SegMasksFromFiji/000_SD_Mask_0_RGBInput_000_00125.tif',\n",
       "  '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/00_InputRaw/8bitSum_Train_00037.tif': '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/11_SegMasksFromFiji/000_SD_Mask_0_RGBInput_000_Train_00037.tif',\n",
       "  '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/00_InputRaw/8bitSum_00631.tif': '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/11_SegMasksFromFiji/000_SD_Mask_0_RGBInput_000_00631.tif',\n",
       "  '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/00_InputRaw/8bitSum_00180.tif': '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/11_SegMasksFromFiji/000_SD_Mask_0_RGBInput_000_00180.tif',\n",
       "  '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/00_InputRaw/8bitSum_00170.tif': '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/11_SegMasksFromFiji/000_SD_Mask_0_RGBInput_000_00170.tif',\n",
       "  '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/00_InputRaw/8bitSum_00206.tif': '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/11_SegMasksFromFiji/000_SD_Mask_0_RGBInput_000_00206.tif',\n",
       "  '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/00_InputRaw/8bitSum_00641.tif': '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/11_SegMasksFromFiji/000_SD_Mask_0_RGBInput_000_00641.tif',\n",
       "  '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/00_InputRaw/8bitSum_00428.tif': '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/11_SegMasksFromFiji/000_SD_Mask_0_RGBInput_000_00428.tif',\n",
       "  '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/00_InputRaw/8bitSum_00596.tif': '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/11_SegMasksFromFiji/000_SD_Mask_0_RGBInput_000_00596.tif',\n",
       "  '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/00_InputRaw/8bitSum_00519.tif': '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/11_SegMasksFromFiji/000_SD_Mask_0_RGBInput_000_00519.tif',\n",
       "  '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/00_InputRaw/8bitSum_Train_00040.tif': '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/11_SegMasksFromFiji/000_SD_Mask_0_RGBInput_000_Train_00040.tif',\n",
       "  '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/00_InputRaw/8bitSum_00060.tif': '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/11_SegMasksFromFiji/000_SD_Mask_0_RGBInput_000_00060.tif',\n",
       "  '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/00_InputRaw/8bitSum_00537.tif': '/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/11_SegMasksFromFiji/000_SD_Mask_0_RGBInput_000_00537.tif'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the info on the files to be imported\n",
    "pair_dic_fn = \"/mnt/ag-microscopy/SampleDataML/EpiCells_Reimport/Processed_005/10_ImportExport/Fiji_FilePairList_SDB2018_EpiCells_005.txt\"\n",
    "mask_master_dic = import_mask_to_img_dics(pair_dic_fn)\n",
    "mask_master_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def results_to_csv(mask_to_img_dic,get_property,root,sub_f,run_ID,output_folder,tag,subset):\n",
    "    '''\n",
    "    Here the results are extracted and saved as csv file.\n",
    "    The naming scheme of the folder Basic_Quantification is as follows:\n",
    "    Combined_Object_$Data_Analysis_ID_$Search_Term_you_used_to_filter_results.csv\n",
    "    Contains all combined results per object.\n",
    "    Results_$Mask_Filename_$Intensity_Image_filename.csv\n",
    "    Contains results per object for the defined pair of images.\n",
    "    Combined_Object_Data_Analysis_ID_$Data_Analysis_ID_$Search_Term_you_used_to_filter_results.csv\n",
    "    Contains all post-processed results per image (e.g. cell number, mean intensity, ect.)\n",
    "    '''\n",
    "    stats_per_folder = []\n",
    "    count = 0\n",
    "    for key,value in mask_to_img_dic.items():\n",
    "        stats_per_img = {}\n",
    "    # load images\n",
    "        ma = tif.imread(key)\n",
    "        im = tif.imread(value)\n",
    "    # get results per object\n",
    "        results = skimage.measure.regionprops_table(ma, intensity_image=im, properties=get_property, cache=True)\n",
    "        results_df = pd.DataFrame.from_records(results)\n",
    "        results_df[\"Mask_Image\"] = os.path.split(key)[1]\n",
    "        results_df[\"Intensity_Image\"] = os.path.split(value)[1]\n",
    "        results_df[\"sum_intensity\"] = results_df[\"mean_intensity\"] * results_df[\"area\"]\n",
    "        results_df[\"circularity\"] = results_df[\"equivalent_diameter\"] * math.pi / results_df[\"perimeter\"]\n",
    "        new_order = [\"Mask_Image\",\"label\",\"area\",\"centroid-0\",\"centroid-1\"] + get_property[4:] + [\"sum_intensity\",\"circularity\",\"Intensity_Image\"]\n",
    "        results_df = results_df.reindex(columns = new_order)\n",
    "        # to avoid confusion for Fiji user\n",
    "        results_df.rename(columns={'centroid-0':'centroid-0_Fiji_Y'}, inplace=True)\n",
    "        results_df.rename(columns={'centroid-1':'centroid-1_Fiji_X'}, inplace=True)\n",
    "        new_fn = \"Results_{}_{}.csv\".format(os.path.split(key)[1],os.path.split(value)[1])\n",
    "        new_fp = os.path.join(root,\"Processed_{}\".format(run_ID),sub_f[output_folder],new_fn)\n",
    "        results_df.to_csv(new_fp, sep=';', decimal=',')\n",
    "    # get results per image\n",
    "        stats_per_img[\"Mask\"] = os.path.split(key)[1]\n",
    "        stats_per_img[\"Intensity_Image\"] = os.path.split(value)[1]\n",
    "        stats_per_img[\"count\"] = results_df.shape[0]\n",
    "        stats_per_img[\"median_area\"] = results_df[\"area\"].median()\n",
    "        stats_per_img[\"mean_area\"] = results_df[\"area\"].mean()\n",
    "        stats_per_img[\"mean_intensity\"] = results_df[\"mean_intensity\"].mean()\n",
    "        stats_per_img[\"median_circularity\"] = results_df[\"circularity\"].median()\n",
    "        stats_per_img[\"median_sum_intensity\"] = results_df[\"sum_intensity\"].median()\n",
    "        stats_per_folder.append(stats_per_img)\n",
    "        if count > 0:\n",
    "            all_data = pd.concat([all_data,results_df])\n",
    "        else:\n",
    "            all_data = results_df\n",
    "        count += 1\n",
    "    # save combined object data\n",
    "    new_fn = \"Combined_Object_Data_{}_{}.csv\".format(\"_\".join(subset),tag)\n",
    "    new_fp = os.path.join(root,\"Processed_{}\".format(run_ID),sub_f[output_folder],new_fn)\n",
    "    all_data.to_csv(new_fp, sep=';', decimal=',')\n",
    "    # save summary data\n",
    "    all_results_df = pd.DataFrame.from_records(stats_per_folder)\n",
    "    new_fn = \"Summary_Results_{}_{}.csv\".format(\"_\".join(subset),tag)\n",
    "    new_fp = os.path.join(root,\"Processed_{}\".format(run_ID),sub_f[output_folder],new_fn)\n",
    "    all_results_df.to_csv(new_fp, sep=';', decimal=',')\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export annditional channel & Quantify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = {}\n",
    "# define here what you want to do\n",
    "pc[\"Export_to_CSV\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pc[\"Export_to_CSV\"]:\n",
    "    all_combined = [] # used for quantifications of more than one intensity channel\n",
    "    # get a list of the masks that were produced by segmentation\n",
    "    mask_files = glob(os.path.join(input_def[\"root\"],\"Processed_{}\".format(run_def[\"run_ID\"]),pc[\"sub_f\"][2])+\"/*.tif\")\n",
    "    mask_to_img_dic, mask_to_8bitimg_dic = make_mask_to_img_dic(mask_files,pc,input_def,run_def,0,pc[\"Intensity_Ch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pc[\"toFiji\"]:\n",
    "    if not pc[\"Export_to_CSV\"]:\n",
    "        mask_files = glob(os.path.join(input_def[\"root\"],\"Processed_{}\".format(run_def[\"run_ID\"]),pc[\"sub_f\"][2])+\"/*.tif\")\n",
    "        mask_to_img_dic, mask_to_8bitimg_dic = make_mask_to_img_dic(mask_files,pc,input_def,run_def,0,pc[\"Intensity_Ch\"])\n",
    "    root_plus = os.path.join(input_def[\"root\"],\"Processed_{}\".format(run_def[\"run_ID\"]))\n",
    "    txt_fn = os.path.join(root_plus,pc[\"sub_f\"][10],\"FilePairList_{}_{}.txt\".format(input_def[\"dataset\"],run_def[\"run_ID\"]))\n",
    "    with open(txt_fn,\"w\") as f:\n",
    "        for mask_fn,image_fn in mask_to_8bitimg_dic.items():\n",
    "            f.write(\"{};{}{}\".format(image_fn.replace(root_plus,\"\"),mask_fn.replace(root_plus,\"\"),\"\\n\"))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export additional channel\n",
    "if pc[\"export_another_channel\"]:\n",
    "    if input_def[\"input_type\"] == \".lif\":\n",
    "        exported_file_list = export_second_channel_for_mask(lifobject,pc,input_def,run_def)\n",
    "    if input_def[\"input_type\"] == \".tif\":\n",
    "        exported_file_list = export_second_channel_for_mask(\"NoneIsTiFF\",pc,input_def,run_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional in case segmentation results shall be filtered by a mask:\n",
    "if pc[\"create_filter_mask_from_channel\"]:\n",
    "    # create new masks (by thresolding the additional input) and extract their names\n",
    "    new_mask_fn_list = create_mask_from_add_ch(exported_file_list,input_def[\"root\"],pc[\"sub_f\"],run_def[\"run_ID\"],run_def[\"para_mp\"],run_def)\n",
    "    # make a dic that has the segmentation output mask name as key, the name of the threshold mask as value\n",
    "    if input_def[\"input_type\"] == \".lif\":\n",
    "        pair_dic = make_pair_second_mask_simple(mask_files,new_mask_fn_list)\n",
    "    if input_def[\"input_type\"] == \".tif\":\n",
    "        core_match = [8,10] # use to define how to match filenames\n",
    "        # for documentation see: how_to_define_core_match.txt\n",
    "        # integrate this variable in OpSeF_Setup!!!\n",
    "        pair_dic = make_pair_second_mask_tiff(mask_files,new_mask_fn_list,core_match)\n",
    "    # create new seqmentation masks per class and return a list of file_names\n",
    "    class1_to_img_dic,class2_to_img_dic = split_by_mask(input_def[\"root\"],run_def[\"run_ID\"],pc[\"sub_f\"],pair_dic,mask_to_8bitimg_dic,mask_to_img_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mask_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pc[\"toFiji\"]:\n",
    "    if pc[\"create_filter_mask_from_channel\"]:\n",
    "        root_plus = os.path.join(input_def[\"root\"],\"Processed_{}\".format(run_def[\"run_ID\"]))\n",
    "        txt_fn = os.path.join(root_plus,pc[\"sub_f\"][10],\"FilePairList_Classes_{}_{}.txt\".format(input_def[\"dataset\"],run_def[\"run_ID\"]))\n",
    "        img_to_class2_dic = dict((v,k) for k,v in class2_to_img_dic.items()) # invert dic 2\n",
    "        with open(txt_fn,\"w\") as f:\n",
    "            for mask_fn,image_fn in class1_to_img_dic.items():\n",
    "                mask2 = img_to_class2_dic[image_fn] # second seg mask\n",
    "                f.write(\"{};{};{};{}\".format(image_fn.replace(root_plus,\"\"),mask_fn.replace(root_plus,\"\"),mask2.replace(root_plus,\"\"),\"\\n\"))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantify original mask\n",
    "if pc[\"Export_to_CSV\"]:\n",
    "    all_combined.append(results_to_csv(mask_to_img_dic,pc[\"get_property\"],input_def[\"root\"],pc[\"sub_f\"],run_def[\"run_ID\"],4,\"All_Main\",input_def[\"subset\"])) # 4 is the main result folder\n",
    "    if pc[\"plot_head_main\"]:\n",
    "        all_combined[0].head()\n",
    "            \n",
    "if pc[\"create_filter_mask_from_channel\"]:\n",
    "    # quantify class1 masks\n",
    "    results_to_csv(class1_to_img_dic,pc[\"get_property\"],input_def[\"root\"],pc[\"sub_f\"],run_def[\"run_ID\"],9,\"Class00\",input_def[\"post_subset\"]) # 9 is the classified result folder\n",
    "    # quantify class2 masks\n",
    "    results_to_csv(class2_to_img_dic,pc[\"get_property\"],input_def[\"root\"],pc[\"sub_f\"],run_def[\"run_ID\"],9,\"Class01\",input_def[\"post_subset\"]) # 9 is the classified result folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pc[\"Quantify_2ndCh\"]:\n",
    "    mask_to_img_dic, mask_to_8bitimg_dic = make_mask_to_img_dic(mask_files,pc,input_def,run_def,5,pc[\"Intensity_2ndCh\"])\n",
    "    all_combined.append(results_to_csv(mask_to_img_dic,pc[\"get_property\"],input_def[\"root\"],pc[\"sub_f\"],run_def[\"run_ID\"],4,\"All_2nd\",input_def[\"subset\"]))\n",
    "    if pc[\"merge_results\"]:\n",
    "        result_summary = merge_intensity_results(all_combined,input_def,pc[\"sub_f\"],run_def,4)\n",
    "        if pc[\"plot_merged\"]:\n",
    "            result_summary.head()\n",
    "else:\n",
    "    if pc[\"Export_to_CSV\"]:\n",
    "        result_summary = all_combined[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AddOn 1: Basic plotting of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pc[\"Plot_Results\"]:\n",
    "    fig, axs = plt.subplots(len(pc[\"Plot_xy\"]), 1, figsize=(5, 5*len(pc[\"Plot_xy\"])), constrained_layout=True)\n",
    "    for i in range(0,len(pc[\"Plot_xy\"])):\n",
    "        axs[i].scatter(result_summary[pc[\"Plot_xy\"][i][0]],result_summary[pc[\"Plot_xy\"][i][1]], c=\"red\")\n",
    "        axs[i].set_title('{} vs {}'.format(*pc[\"Plot_xy\"][i]))\n",
    "        axs[i].set_xlabel(pc[\"Plot_xy\"][i][0],fontsize=15)\n",
    "        axs[i].set_ylabel(pc[\"Plot_xy\"][i][1],fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AddOn 2: Do PCA and TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example pipeline auto-clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pc[\"Cluster_How\"] == \"Auto\":\n",
    "# get data for PCA / TSNE\n",
    "    df_for_tsne_list = extract_values_for_TSNE_PCA(input_def[\"root\"],run_def[\"run_ID\"],pc[\"sub_f\"],4,pc[\"include_in_tsne\"])\n",
    "# get cluster\n",
    "    data = df_for_tsne_list[0].values\n",
    "    auto_clustering = AgglomerativeClustering(linkage=pc[\"link_method\"], n_clusters=pc[\"cluster_expected\"]).fit(data)\n",
    "# do analysis\n",
    "    result_tsne = TSNE(learning_rate=pc[\"tSNE_learning_rate\"]).fit_transform(data)\n",
    "    result_pca = PCA().fit_transform(data)\n",
    "# display results\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 20), constrained_layout=True)\n",
    "    axs[0].scatter(result_tsne[:, 0], result_tsne[:, 1], c=auto_clustering.labels_)\n",
    "    axs[0].set_title('tSNE')\n",
    "    axs[1].scatter(result_pca[:, 0], result_pca[:, 1], c=auto_clustering.labels_)\n",
    "    axs[1].set_title('PCA')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example pipeline mask-clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get data for PCA / TSNE\n",
    "if pc[\"Cluster_How\"] == \"Mask\":\n",
    "    df_for_tsne_list_by_class = extract_values_for_TSNE_PCA(input_def[\"root\"],run_def[\"run_ID\"],pc[\"sub_f\"],9,pc[\"include_in_tsne\"])\n",
    "    fused_df = pd.concat(df_for_tsne_list_by_class,axis = 0,join=\"outer\")\n",
    "    data_by_class = fused_df.values\n",
    "    class_def_by_mask = [0 for x in range (0,df_for_tsne_list_by_class[0].shape[0])] + [1 for x in range (0,df_for_tsne_list_by_class[1].shape[0])]\n",
    "# do analysis\n",
    "    result_tsne_by_class = TSNE(learning_rate=pc[\"tSNE_learning_rate\"]).fit_transform(data_by_class)\n",
    "    result_pca_by_class = PCA().fit_transform(data_by_class)\n",
    "# display results\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 20), constrained_layout=True)\n",
    "    axs[0].scatter(result_tsne_by_class[:, 0], result_tsne_by_class[:, 1], c=class_def_by_mask)\n",
    "    axs[0].set_title('tSNE')\n",
    "    axs[1].scatter(result_pca_by_class[:, 0], result_pca_by_class[:, 1], c=class_def_by_mask)\n",
    "    axs[1].set_title('PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing completed sucessfully !\\n\")\n",
    "print(\"All results have been saved in this folder: \\n\")\n",
    "print(os.path.join(input_def[\"root\"],\"Processed_{}\".format(run_def[\"run_ID\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
