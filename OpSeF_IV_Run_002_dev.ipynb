{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements:\n",
    "\n",
    "Tested with opsef003.yml (see attached file)\n",
    "opsef002 + n2v = opsef003\n",
    "\n",
    "on a GeForce RTX 2080 with 8GB RAM\n",
    "on ubuntu/18.04.3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adaped from:\n",
    "\n",
    "https://github.com/MouseLand/cellpose\n",
    "\n",
    "https://github.com/CellProfiler/CellProfiler\n",
    "\n",
    "https://github.com/mpicbg-csbd/stardist\n",
    "\n",
    "https://github.com/scikit-image/scikit-image\n",
    "\n",
    "https://github.com/VolkerH/unet-nuclei/\n",
    "\n",
    "Thanks to:\n",
    "\n",
    "All developer of the above mentioned repositories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic libs\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import inspect\n",
    "from glob import glob\n",
    "\n",
    "import tifffile as tif\n",
    "\n",
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections\n",
    "import math\n",
    "\n",
    "import pickle\n",
    "\n",
    "import networkx as nx\n",
    "%matplotlib inline\n",
    "\n",
    "# for lif\n",
    "import readlif\n",
    "from readlif.reader import LifFile\n",
    "\n",
    "# skimage\n",
    "import skimage\n",
    "from skimage import transform, io, filters, measure, morphology,img_as_float  \n",
    "from skimage.color import label2rgb,gray2rgb\n",
    "from skimage.filters import gaussian, rank, threshold_otsu\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.measure import label, regionprops, regionprops_table\n",
    "from skimage.morphology import disk, watershed\n",
    "\n",
    "# scipy\n",
    "from scipy.signal import medfilt\n",
    "from scipy.ndimage import generate_binary_structure, binary_dilation\n",
    "\n",
    "# for cellpose\n",
    "from cellpose import models as cp_models\n",
    "from cellpose import utils as cp_utils\n",
    "from cellpose import plot, transforms\n",
    "from cellpose import plot, transforms\n",
    "\n",
    "# other\n",
    "import mxnet as mx\n",
    "\n",
    "# for cluster analysis\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = os.path.dirname(os.path.abspath(inspect.stack()[0][1]))\n",
    "import_path = os.path.join(main_folder,\"Utils_and_Configs\")\n",
    "if import_path not in sys.path:\n",
    "    sys.path.append(import_path)\n",
    "\n",
    "# import from import_path\n",
    "from Tools_002 import *\n",
    "from UNet_CP01 import *\n",
    "from Segmentation_Func_06 import *\n",
    "from Pre_Post_Process002 import *\n",
    "from N2V_DataGeneratorTR001 import *\n",
    "from opsef_core_002 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/mpicbg-csbd/stardist / 3_prediction (2D)\n",
    "from __future__ import print_function, unicode_literals, absolute_import, division\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from csbdeep.utils import Path, normalize\n",
    "from stardist import random_label_cmap, _draw_polygons\n",
    "from stardist.models import StarDist2D\n",
    "\n",
    "# other\n",
    "import pkg_resources\n",
    "import keras\n",
    "\n",
    "# We import all our dependencies.\n",
    "from n2v.models import N2VConfig, N2V\n",
    "from n2v.utils.n2v_utils import manipulate_val_data\n",
    "# from n2v.internals.N2V_DataGenerator2 import N2V_DataGenerator2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load parameter\n",
    "the parameter for processing need to be defined in the notebook.\n",
    "Opsef_Setup_000X\n",
    "this notebook will print in the end a file_path.\n",
    "Please cut and paste it below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./Demo_Notebooks/my_runs/Parameter_muscle_mask_Run_000.pkl\"\n",
    "\n",
    "infile = open(file_path,'rb')\n",
    "parameter = pickle.load(infile)\n",
    "print(\"Loading processing pipeline from\",file_path)\n",
    "infile.close()\n",
    "pc,input_def,run_def,initModelSettings = parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process for all\n",
    "    # create subfolder\n",
    "make_folder_structure(pc,input_def,run_def)\n",
    "\n",
    "# process for lif\n",
    "if input_def[\"input_type\"] == \".lif\":\n",
    "    lifobject,input_def = define_lif_pipeline(input_def)\n",
    "    preprocess_1_for_lif(lifobject,input_def,pc,run_def)\n",
    "    preprocess_2_for_lif(lifobject,input_def,pc,run_def)\n",
    "# process for tif\n",
    "if input_def[\"input_type\"] == \".tif\":\n",
    "    fpath_list = define_tif_pipeline(input_def)\n",
    "    if pc[\"export_another_channel\"]: # implement cleaner\n",
    "        fpath_list = [f for f in fpath_list if input_def[\"export_seg_ch\"] in f]\n",
    "    preprocess_1_for_tif(fpath_list,input_def,pc,run_def)\n",
    "    preprocess_2_for_tif(fpath_list,input_def,pc,run_def)\n",
    "\n",
    "# Segment\n",
    "start_time = datetime.datetime.now()\n",
    "segment(input_def,pc,run_def,initModelSettings)\n",
    "end_time = datetime.datetime.now()\n",
    "time_delta = end_time - start_time\n",
    "print(\"The segmentatio took overall:\", time_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export annditional channel & Quantify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pc[\"Export_to_CSV\"]:\n",
    "    all_combined = [] # used for quantifications of more than one intensity channel\n",
    "    # get a list of the masks that were produced by segmentation\n",
    "    mask_files = glob(os.path.join(input_def[\"root\"],\"Processed_{}\".format(run_def[\"run_ID\"]),pc[\"sub_f\"][2])+\"/*.tif\")\n",
    "    mask_to_img_dic, mask_to_8bitimg_dic = make_mask_to_img_dic(mask_files,pc,input_def,run_def,0,pc[\"Intensity_Ch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pc[\"toFiji\"]:\n",
    "    if not pc[\"Export_to_CSV\"]:\n",
    "        mask_files = glob(os.path.join(input_def[\"root\"],\"Processed_{}\".format(run_def[\"run_ID\"]),pc[\"sub_f\"][2])+\"/*.tif\")\n",
    "        mask_to_img_dic, mask_to_8bitimg_dic = make_mask_to_img_dic(mask_files,pc,input_def,run_def,0,pc[\"Intensity_Ch\"])\n",
    "    root_plus = os.path.join(input_def[\"root\"],\"Processed_{}\".format(run_def[\"run_ID\"]))\n",
    "    txt_fn = os.path.join(root_plus,pc[\"sub_f\"][10],\"FilePairList_{}_{}.txt\".format(input_def[\"dataset\"],run_def[\"run_ID\"]))\n",
    "    with open(txt_fn,\"w\") as f:\n",
    "        for mask_fn,image_fn in mask_to_8bitimg_dic.items():\n",
    "            f.write(\"{};{}{}\".format(image_fn.replace(root_plus,\"\"),mask_fn.replace(root_plus,\"\"),\"\\n\"))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export additional channel\n",
    "if pc[\"export_another_channel\"]:\n",
    "    if input_def[\"input_type\"] == \".lif\":\n",
    "        exported_file_list = export_second_channel_for_mask(lifobject,pc,input_def,run_def)\n",
    "    if input_def[\"input_type\"] == \".tif\":\n",
    "        exported_file_list = export_second_channel_for_mask(\"NoneIsTiFF\",pc,input_def,run_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional in case segmentation results shall be filtered by a mask:\n",
    "if pc[\"create_filter_mask_from_channel\"]:\n",
    "    # create new masks (by thresolding the additional input) and extract their names\n",
    "    new_mask_fn_list = create_mask_from_add_ch(exported_file_list,input_def[\"root\"],pc[\"sub_f\"],run_def[\"run_ID\"],run_def[\"para_mp\"],run_def)\n",
    "    # make a dic that has the segmentation output mask name as key, the name of the threshold mask as value\n",
    "    if input_def[\"input_type\"] == \".lif\":\n",
    "        pair_dic = make_pair_second_mask_simple(mask_files,new_mask_fn_list)\n",
    "    if input_def[\"input_type\"] == \".tif\":\n",
    "        core_match = [8,10] # use to define how to match filenames\n",
    "        # for documentation see: how_to_define_core_match.txt\n",
    "        # integrate this variable in OpSeF_Setup!!!\n",
    "        pair_dic = make_pair_second_mask_tiff(mask_files,new_mask_fn_list,core_match)\n",
    "    # create new seqmentation masks per class and return a list of file_names\n",
    "    class1_to_img_dic,class2_to_img_dic = split_by_mask(input_def[\"root\"],run_def[\"run_ID\"],pc[\"sub_f\"],pair_dic,mask_to_8bitimg_dic,mask_to_img_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mask_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pc[\"toFiji\"]:\n",
    "    if pc[\"create_filter_mask_from_channel\"]:\n",
    "        root_plus = os.path.join(input_def[\"root\"],\"Processed_{}\".format(run_def[\"run_ID\"]))\n",
    "        txt_fn = os.path.join(root_plus,pc[\"sub_f\"][10],\"FilePairList_Classes_{}_{}.txt\".format(input_def[\"dataset\"],run_def[\"run_ID\"]))\n",
    "        img_to_class2_dic = dict((v,k) for k,v in class2_to_img_dic.items()) # invert dic 2\n",
    "        with open(txt_fn,\"w\") as f:\n",
    "            for mask_fn,image_fn in class1_to_img_dic.items():\n",
    "                mask2 = img_to_class2_dic[image_fn] # second seg mask\n",
    "                f.write(\"{};{};{};{}\".format(image_fn.replace(root_plus,\"\"),mask_fn.replace(root_plus,\"\"),mask2.replace(root_plus,\"\"),\"\\n\"))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantify original mask\n",
    "if pc[\"Export_to_CSV\"]:\n",
    "    all_combined.append(results_to_csv(mask_to_img_dic,pc[\"get_property\"],input_def[\"root\"],pc[\"sub_f\"],run_def[\"run_ID\"],4,\"All_Main\",input_def[\"subset\"])) # 4 is the main result folder\n",
    "    if pc[\"plot_head_main\"]:\n",
    "        all_combined[0].head()\n",
    "            \n",
    "if pc[\"create_filter_mask_from_channel\"]:\n",
    "    # quantify class1 masks\n",
    "    results_to_csv(class1_to_img_dic,pc[\"get_property\"],input_def[\"root\"],pc[\"sub_f\"],run_def[\"run_ID\"],9,\"Class00\",input_def[\"post_subset\"]) # 9 is the classified result folder\n",
    "    # quantify class2 masks\n",
    "    results_to_csv(class2_to_img_dic,pc[\"get_property\"],input_def[\"root\"],pc[\"sub_f\"],run_def[\"run_ID\"],9,\"Class01\",input_def[\"post_subset\"]) # 9 is the classified result folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pc[\"Quantify_2ndCh\"]:\n",
    "    mask_to_img_dic, mask_to_8bitimg_dic = make_mask_to_img_dic(mask_files,pc,input_def,run_def,5,pc[\"Intensity_2ndCh\"])\n",
    "    all_combined.append(results_to_csv(mask_to_img_dic,pc[\"get_property\"],input_def[\"root\"],pc[\"sub_f\"],run_def[\"run_ID\"],4,\"All_2nd\",input_def[\"subset\"]))\n",
    "    if pc[\"merge_results\"]:\n",
    "        result_summary = merge_intensity_results(all_combined,input_def,pc[\"sub_f\"],run_def,4)\n",
    "        if pc[\"plot_merged\"]:\n",
    "            result_summary.head()\n",
    "else:\n",
    "    if pc[\"Export_to_CSV\"]:\n",
    "        result_summary = all_combined[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AddOn 1: Basic plotting of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pc[\"Plot_Results\"]:\n",
    "    fig, axs = plt.subplots(len(pc[\"Plot_xy\"]), 1, figsize=(5, 5*len(pc[\"Plot_xy\"])), constrained_layout=True)\n",
    "    for i in range(0,len(pc[\"Plot_xy\"])):\n",
    "        axs[i].scatter(result_summary[pc[\"Plot_xy\"][i][0]],result_summary[pc[\"Plot_xy\"][i][1]], c=\"red\")\n",
    "        axs[i].set_title('{} vs {}'.format(*pc[\"Plot_xy\"][i]))\n",
    "        axs[i].set_xlabel(pc[\"Plot_xy\"][i][0],fontsize=15)\n",
    "        axs[i].set_ylabel(pc[\"Plot_xy\"][i][1],fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AddOn 2: Do PCA and TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example pipeline auto-clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pc[\"Cluster_How\"] == \"Auto\":\n",
    "# get data for PCA / TSNE\n",
    "    df_for_tsne_list = extract_values_for_TSNE_PCA(input_def[\"root\"],run_def[\"run_ID\"],pc[\"sub_f\"],4,pc[\"include_in_tsne\"])\n",
    "# get cluster\n",
    "    data = df_for_tsne_list[0].values\n",
    "    auto_clustering = AgglomerativeClustering(linkage=pc[\"link_method\"], n_clusters=pc[\"cluster_expected\"]).fit(data)\n",
    "# do analysis\n",
    "    result_tsne = TSNE(learning_rate=pc[\"tSNE_learning_rate\"]).fit_transform(data)\n",
    "    result_pca = PCA().fit_transform(data)\n",
    "# display results\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 20), constrained_layout=True)\n",
    "    axs[0].scatter(result_tsne[:, 0], result_tsne[:, 1], c=auto_clustering.labels_)\n",
    "    axs[0].set_title('tSNE')\n",
    "    axs[1].scatter(result_pca[:, 0], result_pca[:, 1], c=auto_clustering.labels_)\n",
    "    axs[1].set_title('PCA')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example pipeline mask-clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get data for PCA / TSNE\n",
    "if pc[\"Cluster_How\"] == \"Mask\":\n",
    "    df_for_tsne_list_by_class = extract_values_for_TSNE_PCA(input_def[\"root\"],run_def[\"run_ID\"],pc[\"sub_f\"],9,pc[\"include_in_tsne\"])\n",
    "    fused_df = pd.concat(df_for_tsne_list_by_class,axis = 0,join=\"outer\")\n",
    "    data_by_class = fused_df.values\n",
    "    class_def_by_mask = [0 for x in range (0,df_for_tsne_list_by_class[0].shape[0])] + [1 for x in range (0,df_for_tsne_list_by_class[1].shape[0])]\n",
    "# do analysis\n",
    "    result_tsne_by_class = TSNE(learning_rate=pc[\"tSNE_learning_rate\"]).fit_transform(data_by_class)\n",
    "    result_pca_by_class = PCA().fit_transform(data_by_class)\n",
    "# display results\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 20), constrained_layout=True)\n",
    "    axs[0].scatter(result_tsne_by_class[:, 0], result_tsne_by_class[:, 1], c=class_def_by_mask)\n",
    "    axs[0].set_title('tSNE')\n",
    "    axs[1].scatter(result_pca_by_class[:, 0], result_pca_by_class[:, 1], c=class_def_by_mask)\n",
    "    axs[1].set_title('PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing completed sucessfully !\\n\")\n",
    "print(\"All results have been saved in this folder: \\n\")\n",
    "print(os.path.join(input_def[\"root\"],\"Processed_{}\".format(run_def[\"run_ID\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
